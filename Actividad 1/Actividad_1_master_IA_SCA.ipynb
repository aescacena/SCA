{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Actividad_1_master_IA_SCA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VFT5_VnJl19"
      },
      "source": [
        "# Actividad 1: Conceptos generales de redes neuronales\n",
        "En esta actividad vamos a revisar algunos de los conceptos basicos de las redes neuronales, pero no por ello menos importantes.\n",
        "\n",
        "El dataset a utilizar es Fashion MNIST, un problema sencillo con imágenes pequeñas de ropa, pero más interesante que el dataset de MNIST. Puedes consultar más información sobre el dataset en este enlace.\n",
        "\n",
        "El código utilizado para contestar tiene que quedar claramente reflejado en el Notebook. Puedes crear nuevas cells si así lo deseas para estructurar tu código y sus salidas. A la hora de entregar el notebook, asegúrate de que los resultados de ejecutar tu código han quedado guardados (por ejemplo, a la hora de entrenar una red neuronal tiene que verse claramente un log de los resultados de cada epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU8gdrsVG4h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5967aa20-7f3e-46d8-a426-830cbf0401ff"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zScMKU2OKSPD"
      },
      "source": [
        "En primer lugar vamos a importar el dataset Fashion MNIST (recordad que este es uno de los dataset de entranamiento que estan guardados en keras) que es el que vamos a utilizar en esta actividad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4voG2hxxG4h3"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphLsCvgKrzb"
      },
      "source": [
        "Llamar a **load_data** en este dataset nos dará dos conjuntos de dos listas, estos serán los valores de entrenamiento y prueba para los gráficos que contienen las prendas de vestir y sus etiquetas.\n",
        "\n",
        "Nota: Aunque en esta actividad lo veis de esta forma, también lo vais a poder encontrar como 4 variables de esta forma: training_images, training_labels, test_images, test_labels = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1muD4PHEG4h6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85566e80-4b6b-40fa-ef9b-ba45e4768fd1"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWGpJqVVLT3Y"
      },
      "source": [
        "Antes de continuar vamos a dar un vistazo a nuestro dataset, para ello vamos a ver una imagen de entrenamiento y su etiqueta o clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5a5PlswG4h8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "97f66843-0bd6-4b9c-9e5b-6461f686e105"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[0], cmap=\"gray\") # recordad que siempre es preferible trabajar en blanco y negro\n",
        "#\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJvZx3MLucY"
      },
      "source": [
        "Habreis notado que todos los valores numericos están entre 0 y 255. Si estamos entrenando una red neuronal, una buena practica es transformar todos los valores entre 0 y 1, un proceso llamado \"normalización\" y afortunadamente en Python es fácil normalizar una lista. Lo puedes hacer de esta manera:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLpAvdVcRlyx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tojL1BmjG4h_"
      },
      "source": [
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUWWsszMAKt"
      },
      "source": [
        "Ahora vamos a definir el modelo, pero antes vamos a repasar algunos comandos y conceptos muy utiles:\n",
        "* **Sequential**: Eso define una SECUENCIA de capas en la red neuronal\n",
        "* **Dense**: Añade una capa de neuronas\n",
        "* **Flatten**: ¿Recuerdas que las imágenes cómo eran las imagenes cuando las imprimiste para poder verlas? Un cuadrado, Flatten sólo toma ese cuadrado y lo convierte en un vector de una dimensión.\n",
        "\n",
        "Cada capa de neuronas necesita una función de activación. Normalmente se usa la función relu en las capas intermedias y softmax en la ultima capa\n",
        "* **Relu** significa que \"Si X>0 devuelve X, si no, devuelve 0\", así que lo que hace es pasar sólo valores 0 o mayores a la siguiente capa de la red.\n",
        "* **Softmax** toma un conjunto de valores, y escoge el más grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgBW1yE2MwPp"
      },
      "source": [
        " **Pregunta 1 (3.5 puntos)**. Utilizando Keras, y preparando los datos de X e y como fuera necesario, define y entrena una red neuronal que sea capaz de clasificar imágenes de Fashion MNIST con las siguientes características:\n",
        "\n",
        "* Una hidden layer de tamaños 128, utilizando unidades sigmoid\n",
        "Optimizador Adam.\n",
        "* Durante el entrenamiento, la red tiene que mostrar resultados de loss y accuracy por cada epoch.\n",
        "* La red debe entrenar durante 10 epochs y batch size de 64.\n",
        "* La última capa debe de ser una capa softmax.\n",
        "* Tu red tendría que ser capaz de superar fácilmente 80% de accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTaD2QXIORwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c4b102-a943-4d93-dc73-1d7cd1deaffb"
      },
      "source": [
        "### Tu código para la red neuronal de la pregunta 1 aquí ###\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model.add(keras.layers.Dense(128, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 10 neuronas, las 10 posibles prendas\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bh_ACMrr9tI",
        "outputId": "f259eb9a-69fd-4ff9-a847-1542fabac43e"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "X_valid, X_train = training_images[:5000], training_images[5000:]\n",
        "y_valid, y_train = training_labels[:5000], training_labels[5000:]\n",
        "\n",
        "# Entrenamos el modelo\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), batch_size=64)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "860/860 [==============================] - 4s 4ms/step - loss: 0.1734 - accuracy: 0.9375 - val_loss: 0.2890 - val_accuracy: 0.8952\n",
            "Epoch 2/20\n",
            "860/860 [==============================] - 3s 4ms/step - loss: 0.1694 - accuracy: 0.9387 - val_loss: 0.2862 - val_accuracy: 0.8988\n",
            "Epoch 3/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1654 - accuracy: 0.9405 - val_loss: 0.2936 - val_accuracy: 0.8940\n",
            "Epoch 4/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1617 - accuracy: 0.9416 - val_loss: 0.2961 - val_accuracy: 0.8970\n",
            "Epoch 5/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1580 - accuracy: 0.9440 - val_loss: 0.2974 - val_accuracy: 0.8988\n",
            "Epoch 6/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1548 - accuracy: 0.9445 - val_loss: 0.2908 - val_accuracy: 0.8978\n",
            "Epoch 7/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1500 - accuracy: 0.9466 - val_loss: 0.2965 - val_accuracy: 0.8980\n",
            "Epoch 8/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1479 - accuracy: 0.9475 - val_loss: 0.2954 - val_accuracy: 0.8976\n",
            "Epoch 9/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1443 - accuracy: 0.9486 - val_loss: 0.3033 - val_accuracy: 0.8974\n",
            "Epoch 10/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1410 - accuracy: 0.9501 - val_loss: 0.3011 - val_accuracy: 0.8948\n",
            "Epoch 11/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1378 - accuracy: 0.9517 - val_loss: 0.2984 - val_accuracy: 0.8976\n",
            "Epoch 12/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1344 - accuracy: 0.9522 - val_loss: 0.3151 - val_accuracy: 0.8946\n",
            "Epoch 13/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1316 - accuracy: 0.9538 - val_loss: 0.3166 - val_accuracy: 0.8976\n",
            "Epoch 14/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1281 - accuracy: 0.9550 - val_loss: 0.3027 - val_accuracy: 0.8978\n",
            "Epoch 15/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1253 - accuracy: 0.9559 - val_loss: 0.3012 - val_accuracy: 0.8990\n",
            "Epoch 16/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1226 - accuracy: 0.9575 - val_loss: 0.3042 - val_accuracy: 0.8984\n",
            "Epoch 17/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1220 - accuracy: 0.9568 - val_loss: 0.3100 - val_accuracy: 0.8972\n",
            "Epoch 18/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1176 - accuracy: 0.9585 - val_loss: 0.3232 - val_accuracy: 0.8968\n",
            "Epoch 19/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1146 - accuracy: 0.9606 - val_loss: 0.3141 - val_accuracy: 0.8958\n",
            "Epoch 20/20\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1126 - accuracy: 0.9605 - val_loss: 0.3330 - val_accuracy: 0.8864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "izvIcrDRW-Ox",
        "outputId": "9753f4f2-7871-482b-e2fe-3f5470b4ae33"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxr5hTKYOQnK"
      },
      "source": [
        "Para concluir el entrenamiento de la red neuronal, una buena practica es evaluar el modelo para ver si la precisión de entrenamiento es real\n",
        "\n",
        "**pregunta 2 (0.5 puntos)**: evalua el modelo con las imagenes y etiquetas test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNjQEtUUG4iI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef613d2-6de0-4246-a3f7-8e2467f3a86d"
      },
      "source": [
        "### Tu código para la evaluación de la red neuronal de la pregunta 2 aquí ###\n",
        "\n",
        "model.evaluate(test_images, test_labels) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8773\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37855109572410583, 0.8773000240325928]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GOzh639aCxk"
      },
      "source": [
        "Podemos ver que en la última iteración del entrenamiendo del modelo tenemos:\n",
        "\n",
        "1.   loss: 0.1126\n",
        "2.   accuracy: 0.9605\n",
        "\n",
        "Tras evaluar el modelo con los valores de test podemos considerar que está bien entrenado ya que tenemos unos valores semejantes:\n",
        "\n",
        "1.   loss: 0.3786\n",
        "2.   accuracy: 0.8773"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygMVnmSYO83U"
      },
      "source": [
        "Ahora vamos a explorar el código con una serie de ejercicios para alcanzar un grado de comprensión mayor sobre las redes neuronales y su entrenamiento.\n",
        "\n",
        "# **Ejercicio 1: Funcionamiento de las predicción de la red neuronal**\n",
        "\n",
        "Para este primer ejercicio sigue los siguientes pasos: \n",
        "\n",
        "* Crea una variable llamada **classifications** para construir un clasificador para las imágenes de prueba, para ello puedes utilizar la función predict sobre el conjunto de test\n",
        "* Imprime con la función print la primera entrada en las clasificaciones. \n",
        "\n",
        "**pregunta 3.1 (0.25 puntos)**, el resultado al imprimirlo es un vector de números, \n",
        "* ¿Por qué crees que ocurre esto, y qué representa este vector de números?\n",
        "\n",
        "**pregunta 3.2 (0.25 puntos)**\n",
        "* ¿Cúal es la clase de la primera entrada#  de la variable **classifications**? La respuesta puede ser un número o su etiqueta/clase equivalente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mL-h4xQhCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c56419-b78d-4845-b1d4-ad572b793a2d"
      },
      "source": [
        "### Tu código del clasificador de la pregunta 3 aquí ###\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(np.argmax(classifications[0]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.8277669e-11 3.8395145e-10 1.6089382e-09 3.1912883e-10 4.3480936e-10 6.2448089e-06 1.3633886e-08 9.2139933e-04 1.1584957e-11 9.9907231e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvbVC9gaQhMY"
      },
      "source": [
        "Tu respuesta a la pregunta 3.1 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylXRWrKSe_KW"
      },
      "source": [
        "Representa el nivel de **confianza** del modelo sobre las imagenes de prendas de ropa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRWo-75tdgv0"
      },
      "source": [
        "Tu respuesta a la pregunta 3.2 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSgYued-fofW"
      },
      "source": [
        "Clase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQ8qAzhRQ4L"
      },
      "source": [
        "# **Ejercicio 2: Impacto variar el número de neuronas en las capas ocultas**\n",
        "\n",
        "En este ejercicio vamos a experimentar con nuestra red neuronal cambiando el numero de neuronas por 512 y por 1024. Para ello, utiliza la red neuronal de la pregunta 1, y su capa oculta cambia las 128 neuronas:\n",
        "\n",
        "* **pregunta 4.1 (0.25 puntos)**: 512 neuronas en la capa oculta\n",
        "* **pregunta 4.2 (0.25 puntos)**:1024 neuronas en la capa oculta\n",
        "\n",
        "y entrena la red en ambos casos.\n",
        "\n",
        "**pregunta 4.3 (0.5 puntos)**: ¿Cual es el impacto que tiene la red neuronal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdP8ZwuaUV93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e53e5c-fad4-4b08-89a0-dab86cb695a2"
      },
      "source": [
        "### Tu código para 512 neuronas aquí ###\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model_512 = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model_512.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model_512.add(keras.layers.Dense(512, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 10 neuronas, las 10 posibles prendas\n",
        "model_512.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model_512.summary()\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_512.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Entrenamos el modelo\n",
        "history = model_512.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), batch_size=64)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_11 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "860/860 [==============================] - 6s 6ms/step - loss: 0.5366 - accuracy: 0.8108 - val_loss: 0.4325 - val_accuracy: 0.8454\n",
            "Epoch 2/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.4072 - accuracy: 0.8538 - val_loss: 0.3868 - val_accuracy: 0.8606\n",
            "Epoch 3/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.3673 - accuracy: 0.8672 - val_loss: 0.3738 - val_accuracy: 0.8650\n",
            "Epoch 4/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.3418 - accuracy: 0.8754 - val_loss: 0.3541 - val_accuracy: 0.8706\n",
            "Epoch 5/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.3201 - accuracy: 0.8833 - val_loss: 0.3272 - val_accuracy: 0.8800\n",
            "Epoch 6/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.3035 - accuracy: 0.8875 - val_loss: 0.3169 - val_accuracy: 0.8848\n",
            "Epoch 7/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2874 - accuracy: 0.8942 - val_loss: 0.3069 - val_accuracy: 0.8862\n",
            "Epoch 8/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2756 - accuracy: 0.8980 - val_loss: 0.3051 - val_accuracy: 0.8884\n",
            "Epoch 9/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2628 - accuracy: 0.9022 - val_loss: 0.3349 - val_accuracy: 0.8806\n",
            "Epoch 10/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2521 - accuracy: 0.9065 - val_loss: 0.3106 - val_accuracy: 0.8912\n",
            "Epoch 11/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2402 - accuracy: 0.9103 - val_loss: 0.2996 - val_accuracy: 0.8906\n",
            "Epoch 12/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2305 - accuracy: 0.9136 - val_loss: 0.2987 - val_accuracy: 0.8900\n",
            "Epoch 13/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2228 - accuracy: 0.9166 - val_loss: 0.2893 - val_accuracy: 0.8952\n",
            "Epoch 14/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2125 - accuracy: 0.9206 - val_loss: 0.3060 - val_accuracy: 0.8886\n",
            "Epoch 15/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.2045 - accuracy: 0.9241 - val_loss: 0.3038 - val_accuracy: 0.8902\n",
            "Epoch 16/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.1961 - accuracy: 0.9281 - val_loss: 0.2872 - val_accuracy: 0.8958\n",
            "Epoch 17/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.1895 - accuracy: 0.9307 - val_loss: 0.2871 - val_accuracy: 0.8996\n",
            "Epoch 18/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.1828 - accuracy: 0.9317 - val_loss: 0.2919 - val_accuracy: 0.8942\n",
            "Epoch 19/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.1741 - accuracy: 0.9350 - val_loss: 0.2886 - val_accuracy: 0.8972\n",
            "Epoch 20/20\n",
            "860/860 [==============================] - 5s 6ms/step - loss: 0.1692 - accuracy: 0.9371 - val_loss: 0.2941 - val_accuracy: 0.8972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXBlbbfuUaPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02eff184-6cee-4004-eed3-c30c32b80f1f"
      },
      "source": [
        "### Tu código para 1024 neuronas aquí ###\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model_1024 = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model_1024.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model_1024.add(keras.layers.Dense(1024, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 10 neuronas, las 10 posibles prendas\n",
        "model_1024.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model_1024.summary()\n",
        "\n",
        "# Compilamos el modelo\n",
        "model_1024.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Entrenamos el modelo\n",
        "history = model_1024.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), batch_size=64)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_12 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 814,090\n",
            "Trainable params: 814,090\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.5304 - accuracy: 0.8115 - val_loss: 0.4134 - val_accuracy: 0.8596\n",
            "Epoch 2/20\n",
            "860/860 [==============================] - 8s 10ms/step - loss: 0.4079 - accuracy: 0.8529 - val_loss: 0.3724 - val_accuracy: 0.8644\n",
            "Epoch 3/20\n",
            "860/860 [==============================] - 8s 10ms/step - loss: 0.3696 - accuracy: 0.8648 - val_loss: 0.3597 - val_accuracy: 0.8686\n",
            "Epoch 4/20\n",
            "860/860 [==============================] - 8s 10ms/step - loss: 0.3426 - accuracy: 0.8753 - val_loss: 0.3380 - val_accuracy: 0.8772\n",
            "Epoch 5/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.3197 - accuracy: 0.8824 - val_loss: 0.3283 - val_accuracy: 0.8806\n",
            "Epoch 6/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.3010 - accuracy: 0.8888 - val_loss: 0.3123 - val_accuracy: 0.8882\n",
            "Epoch 7/20\n",
            "860/860 [==============================] - 8s 10ms/step - loss: 0.2826 - accuracy: 0.8952 - val_loss: 0.3184 - val_accuracy: 0.8836\n",
            "Epoch 8/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2686 - accuracy: 0.8994 - val_loss: 0.2944 - val_accuracy: 0.8920\n",
            "Epoch 9/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2565 - accuracy: 0.9039 - val_loss: 0.3045 - val_accuracy: 0.8910\n",
            "Epoch 10/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2430 - accuracy: 0.9103 - val_loss: 0.3135 - val_accuracy: 0.8870\n",
            "Epoch 11/20\n",
            "860/860 [==============================] - 8s 10ms/step - loss: 0.2306 - accuracy: 0.9129 - val_loss: 0.2897 - val_accuracy: 0.8924\n",
            "Epoch 12/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2209 - accuracy: 0.9174 - val_loss: 0.2870 - val_accuracy: 0.8950\n",
            "Epoch 13/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2101 - accuracy: 0.9217 - val_loss: 0.3186 - val_accuracy: 0.8860\n",
            "Epoch 14/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.2010 - accuracy: 0.9255 - val_loss: 0.2864 - val_accuracy: 0.8986\n",
            "Epoch 15/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1919 - accuracy: 0.9283 - val_loss: 0.3004 - val_accuracy: 0.8910\n",
            "Epoch 16/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1842 - accuracy: 0.9309 - val_loss: 0.3023 - val_accuracy: 0.8956\n",
            "Epoch 17/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1772 - accuracy: 0.9336 - val_loss: 0.3096 - val_accuracy: 0.8914\n",
            "Epoch 18/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1683 - accuracy: 0.9374 - val_loss: 0.3042 - val_accuracy: 0.8926\n",
            "Epoch 19/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1599 - accuracy: 0.9405 - val_loss: 0.3084 - val_accuracy: 0.8948\n",
            "Epoch 20/20\n",
            "860/860 [==============================] - 9s 10ms/step - loss: 0.1515 - accuracy: 0.9436 - val_loss: 0.3063 - val_accuracy: 0.8982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG0h2HL-Uj93"
      },
      "source": [
        "Tu respuesta a la pregunta 4.3 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulIoB5GUjNAH"
      },
      "source": [
        "\n",
        "Como podemos ver en los valores de la última iteración del entrenamiento del modelo, estos devuelven resultados muy parecidos incluso aumentando neuronas.\n",
        "\n",
        "*   128 Neuronas: loss: 0.1126 - accuracy: 0.9605 - val_loss: 0.3330 - val_accuracy: 0.8864\n",
        "*   512 Neuronas: loss: 0.1695 - accuracy: 0.9382 - val_loss: 0.3088 - val_accuracy: 0.8896\n",
        "*   1024 Neuronas: loss: 0.1553 - accuracy: 0.9413 - val_loss: 0.3117 - val_accuracy: 0.8948\n",
        "\n",
        "Al parece son capaces de predecir casi con la misma precisión, la gran diferencia entre ellos está en los parámetros donde:\n",
        "\n",
        "*   128 Neuronas: 101 parámetros\n",
        "*   512 Neuronas: 407 parámetros\n",
        "*   1024 Neuronas: 814 parámetros\n",
        "\n",
        "Con lo que en conclusión no necesitamos más neuronas para mejorar la predicción, solo conseguimos necesitar más tiempo para entrenar el modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-NpUI9EVkVz"
      },
      "source": [
        "Si ahora entrenais el modelo de esta forma (con 512 y 1024 neuronas en la capa oculta) y volveis a ejecutar el predictor guardado en la variable **classifications**, escribir el código del clasificador del ejercicio 1 de nuevo e imprimid el primer objeto guardado en la variable classifications.\n",
        "\n",
        "**pregunta 5.1 (0.25 puntos)**: \n",
        "\n",
        "* ¿En que clase esta clasificado ahora la primera prenda de vestir de la variable classifications?\n",
        "\n",
        "**pregunta 5.1 (0.25 puntos)**: \n",
        "\n",
        "* ¿Porque crees que ha ocurrido esto?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdJHl3V-G4iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910f4e08-e5ba-4c4b-f799-2fe7dd9e8042"
      },
      "source": [
        "### Tu código del clasificador de la pregunta 5 aquí ###\n",
        "\n",
        "classifications = model_512.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(np.argmax(classifications[0]))\n",
        "\n",
        "classifications = model_1024.predict(test_images)\n",
        "print(classifications[0])\n",
        "print(np.argmax(classifications[0]))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.9554391e-09 1.4146716e-09 7.7721953e-09 7.8955792e-10 1.9942927e-08 9.4262501e-03 2.0829210e-08 1.2611136e-03 3.2429643e-08 9.8931259e-01]\n",
            "9\n",
            "[1.2702189e-09 2.2472886e-10 5.5661575e-10 2.1863412e-11 5.5513460e-10 6.5829357e-05 3.3182459e-09 1.7740599e-04 3.1353153e-11 9.9975675e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3NfwdOGZcAa"
      },
      "source": [
        "Tu respuesta a la pregunta 5.1 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_4rJ35ap2qX"
      },
      "source": [
        "En ambos predictores clasifica con la clase 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFmfpxE1ZcJx"
      },
      "source": [
        "Tu respuesta a la pregunta 5.2 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzXbG13p-Ou"
      },
      "source": [
        "Creo que predice con la misma clase ya que los valores de loss y accuracy son casi identicos en los 3 modelos con diferentes neuronas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59eM76O1YekZ"
      },
      "source": [
        "# **Ejercicio 3: ¿por qué es tan importante la capa Flatten?**\n",
        "\n",
        "En este ejercicio vamos a ver que ocurre cuando quitamos la capa flatten, para ello, escribe la red neuronal de la pregunta 1 y no pongas la capa Flatten.\n",
        "\n",
        "**pregunta 6 (0.5 puntos):** ¿puedes explicar porque da el error que da?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecfEVKEuG4iU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "52a2e281-1cd9-4ba9-862a-37478a639344"
      },
      "source": [
        "### Tu código de la red neuronal sin capa flatten de la pregunta 6 aquí ###\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# Capa 1\n",
        "model.add(keras.layers.Dense(128, activation=\"sigmoid\"))\n",
        "# Capa 2: Con 10 neuronas, las 10 posibles prendas\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-421b9f0aa449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Capa 2: Con 10 neuronas, las 10 posibles prendas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \"\"\"\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2522\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNmrkkOZN6D"
      },
      "source": [
        "Tu respuesta a la pregunta 6 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4bGLVVYuFvw"
      },
      "source": [
        "Por que estamos usando redes neuronales clásicas y estas necesitan recibir los valores de forma escalar, osea pixel a pixel (1 dimensión)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37cIr81ZYJj"
      },
      "source": [
        "# **Ejercicio 4: Número de neuronas de la capa de salida**\n",
        "Considerad la capa final, la de salida de la red neuronal de la pregunta 1.\n",
        "\n",
        "**pregunta 7.1 (0.25 puntos)**: ¿Por qué son 10 las neuronas de la última capa?\n",
        "\n",
        "**pregunta 7.2 (0.25 puntos)**: ¿Qué pasaría si tuvieras una cantidad diferente a 10? \n",
        "\n",
        "Por ejemplo, intenta entrenar la red con 5, para ello utiliza la red neuronal de la pregunta 1 y cambia a 5 el número de neuronas en la última capa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhbZkppYZOCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "af633df0-060f-49a1-9678-c2931a908105"
      },
      "source": [
        "### Tu código de la red neuronal con 5 neuronas en la capa de salida de la pregunta 7 aquí ###\n",
        "\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model.add(keras.layers.Dense(128, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 5 neuronas, las 10 posibles prendas\n",
        "model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Entrenamos el modelo\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), batch_size=64)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_28 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 101,125\n",
            "Trainable params: 101,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ece9779fc8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Entrenamos el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 9 which is outside the valid range of [0, 5).  Label values: 5 7 0 7 7 1 9 8 4 5 4 0 4 4 5 0 4 7 5 7 2 3 5 4 8 4 7 3 0 2 4 5 7 4 3 8 4 0 6 2 3 2 5 3 7 4 9 1 1 5 4 7 2 5 0 6 3 5 8 1 7 6 4 4\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-55-ece9779fc8e3>:14) ]] [Op:__inference_train_function_562644]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLsQcq-6aUoD"
      },
      "source": [
        "Tu respuestas a la pregunta 7.1 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySogXCZcwdW5"
      },
      "source": [
        "Cada nodo contiene una calificacion que indica la probabilidad que la actual imagen pertenece a una de las 10 clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1f_7ZFeaUu6"
      },
      "source": [
        "Tu respuestas a la pregunta 7.2 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awswqA4SxnnM"
      },
      "source": [
        "Al entrenar el modelo nos lanza un error indicando que:\n",
        "\n",
        "Recibió un valor de etiqueta de 9 que está fuera del rango válido de [0, 5).\n",
        "\n",
        "Con lo cual no podemos predecir con 5 neuronas de salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIBCkshaf2y"
      },
      "source": [
        "# Ejercicio 5: Aumento de epoch y su efecto en la red neuronal\n",
        "En este ejercicio vamos a ver el impacto de aumentar los epoch en el entrenamiento. Usando la red neuronal de la pregunta 1:\n",
        "\n",
        "**pregunta 8.1 (0.20 puntos)**\n",
        "* Intentad 15 epoch para su entrenamiento, probablemente obtendras un modelo con una pérdida mucho mejor que el que tiene 5.\n",
        "\n",
        "**pregunta 8.2 (0.20 puntos)**\n",
        "* Intenta ahora con 30 epoch para su entrenamiento, podrás ver que el valor de la pérdida deja de disminuir, y a veces aumenta.\n",
        "\n",
        "**pregunta 8.3 (0.60 puntos)**\n",
        "* ¿Porque que piensas que ocurre esto? Explica tu respuesta y da el nombre de este efecto si lo conoces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5vk_imG4iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b0d519-44c2-4e6c-8be1-70d3e67b760f"
      },
      "source": [
        "### Tu código para 15 epoch aquí ###\n",
        "\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model.add(keras.layers.Dense(128, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 10 neuronas, las 10 posibles prendas\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Entrenamos el modelo\n",
        "history = model.fit(X_train, y_train, epochs=15, validation_data=(X_valid, y_valid), batch_size=64)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_23 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.6158 - accuracy: 0.7941 - val_loss: 0.4397 - val_accuracy: 0.8486\n",
            "Epoch 2/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.4201 - accuracy: 0.8499 - val_loss: 0.3997 - val_accuracy: 0.8558\n",
            "Epoch 3/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3801 - accuracy: 0.8639 - val_loss: 0.3700 - val_accuracy: 0.8698\n",
            "Epoch 4/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3552 - accuracy: 0.8721 - val_loss: 0.3461 - val_accuracy: 0.8722\n",
            "Epoch 5/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3367 - accuracy: 0.8780 - val_loss: 0.3475 - val_accuracy: 0.8732\n",
            "Epoch 6/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3212 - accuracy: 0.8843 - val_loss: 0.3460 - val_accuracy: 0.8744\n",
            "Epoch 7/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3083 - accuracy: 0.8880 - val_loss: 0.3405 - val_accuracy: 0.8770\n",
            "Epoch 8/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2965 - accuracy: 0.8919 - val_loss: 0.3179 - val_accuracy: 0.8852\n",
            "Epoch 9/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2866 - accuracy: 0.8952 - val_loss: 0.3230 - val_accuracy: 0.8820\n",
            "Epoch 10/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2768 - accuracy: 0.8991 - val_loss: 0.3193 - val_accuracy: 0.8820\n",
            "Epoch 11/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2707 - accuracy: 0.9005 - val_loss: 0.3055 - val_accuracy: 0.8876\n",
            "Epoch 12/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2612 - accuracy: 0.9044 - val_loss: 0.3057 - val_accuracy: 0.8916\n",
            "Epoch 13/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2535 - accuracy: 0.9078 - val_loss: 0.3182 - val_accuracy: 0.8848\n",
            "Epoch 14/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2464 - accuracy: 0.9096 - val_loss: 0.2959 - val_accuracy: 0.8918\n",
            "Epoch 15/15\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2395 - accuracy: 0.9122 - val_loss: 0.2989 - val_accuracy: 0.8896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9jQ26Gda5cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b139dfdd-435a-4c1a-f694-0725e6e29d24"
      },
      "source": [
        "### Tu código para 30 epoch aquí ###\n",
        "\n",
        "# Creamos un modelo conformado por una secuencia de capas\n",
        "model = keras.models.Sequential()\n",
        "# Capa 1\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
        "# Capa 2\n",
        "model.add(keras.layers.Dense(128, activation=\"sigmoid\"))\n",
        "# Capa 3: Con 10 neuronas, las 10 posibles prendas\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Entrenamos el modelo\n",
        "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), batch_size=64)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_29 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f3d1bd72440>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "860/860 [==============================] - 113s 3ms/step - loss: 0.6089 - accuracy: 0.7977 - val_loss: 0.4396 - val_accuracy: 0.8462\n",
            "Epoch 2/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.4191 - accuracy: 0.8507 - val_loss: 0.3877 - val_accuracy: 0.8632\n",
            "Epoch 3/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3799 - accuracy: 0.8623 - val_loss: 0.3650 - val_accuracy: 0.8690\n",
            "Epoch 4/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3545 - accuracy: 0.8724 - val_loss: 0.3508 - val_accuracy: 0.8722\n",
            "Epoch 5/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3360 - accuracy: 0.8787 - val_loss: 0.3396 - val_accuracy: 0.8744\n",
            "Epoch 6/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3195 - accuracy: 0.8844 - val_loss: 0.3309 - val_accuracy: 0.8778\n",
            "Epoch 7/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.3069 - accuracy: 0.8886 - val_loss: 0.3322 - val_accuracy: 0.8774\n",
            "Epoch 8/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2954 - accuracy: 0.8927 - val_loss: 0.3188 - val_accuracy: 0.8838\n",
            "Epoch 9/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2847 - accuracy: 0.8965 - val_loss: 0.3209 - val_accuracy: 0.8854\n",
            "Epoch 10/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2759 - accuracy: 0.8993 - val_loss: 0.3247 - val_accuracy: 0.8830\n",
            "Epoch 11/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2668 - accuracy: 0.9018 - val_loss: 0.3151 - val_accuracy: 0.8872\n",
            "Epoch 12/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2591 - accuracy: 0.9061 - val_loss: 0.3035 - val_accuracy: 0.8908\n",
            "Epoch 13/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2510 - accuracy: 0.9093 - val_loss: 0.3055 - val_accuracy: 0.8894\n",
            "Epoch 14/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2446 - accuracy: 0.9109 - val_loss: 0.3016 - val_accuracy: 0.8872\n",
            "Epoch 15/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2377 - accuracy: 0.9133 - val_loss: 0.2937 - val_accuracy: 0.8930\n",
            "Epoch 16/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2310 - accuracy: 0.9152 - val_loss: 0.2981 - val_accuracy: 0.8938\n",
            "Epoch 17/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2268 - accuracy: 0.9171 - val_loss: 0.2904 - val_accuracy: 0.8926\n",
            "Epoch 18/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2213 - accuracy: 0.9201 - val_loss: 0.2932 - val_accuracy: 0.8940\n",
            "Epoch 19/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2147 - accuracy: 0.9221 - val_loss: 0.2937 - val_accuracy: 0.8950\n",
            "Epoch 20/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2095 - accuracy: 0.9233 - val_loss: 0.2914 - val_accuracy: 0.8962\n",
            "Epoch 21/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.2038 - accuracy: 0.9263 - val_loss: 0.2886 - val_accuracy: 0.8956\n",
            "Epoch 22/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1999 - accuracy: 0.9280 - val_loss: 0.2934 - val_accuracy: 0.8922\n",
            "Epoch 23/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1963 - accuracy: 0.9284 - val_loss: 0.2869 - val_accuracy: 0.8940\n",
            "Epoch 24/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1894 - accuracy: 0.9323 - val_loss: 0.2915 - val_accuracy: 0.8926\n",
            "Epoch 25/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1868 - accuracy: 0.9331 - val_loss: 0.2939 - val_accuracy: 0.8992\n",
            "Epoch 26/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1817 - accuracy: 0.9345 - val_loss: 0.2925 - val_accuracy: 0.8970\n",
            "Epoch 27/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1775 - accuracy: 0.9359 - val_loss: 0.2899 - val_accuracy: 0.8978\n",
            "Epoch 28/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1726 - accuracy: 0.9373 - val_loss: 0.3001 - val_accuracy: 0.8954\n",
            "Epoch 29/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1693 - accuracy: 0.9394 - val_loss: 0.2947 - val_accuracy: 0.8928\n",
            "Epoch 30/30\n",
            "860/860 [==============================] - 3s 3ms/step - loss: 0.1667 - accuracy: 0.9401 - val_loss: 0.3028 - val_accuracy: 0.8964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0fjzH4bmSR"
      },
      "source": [
        "Tu respuesta a la pregunta 8.3 aquí:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCMnOkx03hWM"
      },
      "source": [
        "En ciertos pasos el algoritmo se aleja de la función verdadera, aumentando de manera previsible el error de predicción sobre los datos de validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlIgNG4Yb_N6"
      },
      "source": [
        "# Ejercicio 6: Early stop\n",
        "En el ejercicio anterior, cuando entrenabas con epoch extras, tenías un problema en el que tu pérdida podía cambiar. Puede que te haya llevado un poco de tiempo esperar a que el entrenamiento lo hiciera,  y puede que hayas pensado \"¿no estaría bien si pudiera parar el entrenamiento cuando alcance un valor deseado?\", es decir, una precisión del 85% podría ser suficiente para ti, y si alcanzas eso después de 3 epoch, ¿por qué sentarte a esperar a que termine muchas más épocas? Como cualquier otro programa existen formas de parar la ejecución\n",
        "\n",
        "A partir del ejemplo de código que\n",
        "\n",
        " se da, hacer una nueva función que tenga en cuenta la perdida (loss) y que pueda parar el código para\n",
        "evitar que ocurra el efeto secundario que vimos en el ejercicio 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5UwceFUG4ic"
      },
      "source": [
        "### Ejemplo de código\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy')> 0.85):\n",
        "              print(\"\\nAlcanzado el 85% de precisión, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bjd8wGKccrn"
      },
      "source": [
        "**Pregunta 9 (2 puntos)**: Completa el siguiente código con una clase callback que una vez alcanzado el 40% de perdida detenga el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29LSfdOvc270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1a0dc6-1597-4426-9c8c-82eaaabd9efd"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "### Tu código de la función callback para parar el entrenamiento de la red neuronal al 40% de loss aqui: ###\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('loss')> 0.40):\n",
        "              print(\"\\nAlcanzado el 40% de loss, se cancela el entrenamiento!!\")\n",
        "              self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy']) \n",
        "\n",
        "model.fit(training_images, training_labels, epochs=50, callbacks=[callbacks])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n",
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4742 - accuracy: 0.8301\n",
            "\n",
            "Alcanzado el 40% de loss, se cancela el entrenamiento!!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cf25b1c10>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0sIOPha4P2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}